---
layout: post
title:  "Reading Reflection #4: Interpretation"
date:   "2022-10-31"
categories: posts
---

Reading the article in Hermeneutica was interesting because it was both informative about the different ways that Voyant tools can provide insights about a text, but it also posed some interesting questions about the nature of interpretation in the humanities, and what we can actually use these tools for and what we can’t use them for. Firstly, the article was a good primer for using Voyant. I felt like I got a good understanding of some of the main uses of Voyant. These included the use of word clouds to understand recurring words in a text, using cross-corpus or cross-text analysis to look at how different words appear at different rates between texts, and using graphs to chart uses of words within a text. I am sure there are many more ways to use Voyant besides the ways described, but it was a good way to get situated. I think the more important takeaway from the reading, though, was the use of computers in interpreting humanities texts. When we use a computer to analyze a text, what are we actually doing? They bring up ideas like Searle’ Chinese Room to explain that computers, as we see them today, aren’t doing any “understanding” of text or information, but they are purely algorithmic ways of sorting information to present them in a way for the human analyst to understand. With this comes a really important message in the use of computer tools in the humanities: we cannot look at data outputs and consider them interpretations, but we must use the tools as a way of constructing our own interpretations with the tools as a guide. Ascribing some kind of built in understanding to the computer’s analysis may lead our own understandings astray, and to use these tools we must understand how the computer works, so we don’t assign it any kind of interpretative ideas that may not actually exist under the hood.
	One question that I have concerning the use of tools like Voyant is, how much can we actually learn from these tools without some kind of computer interpretation? When we see how the use of a word changes throughout a text or between texts, can we use that data as evidence for some kind of phenomenon, just based on the data alone? I feel as though that these tools are not enough for understanding the data on a valuable level, but are really more just interesting ways at looking at a particular phenomenon, but not really being able to use them as evidence or something that might be studied in itself. How might these tools be able to provide real evidence for study, if they are uninterpreted data collection? I.e, knowing how many times a word appears in a chapter may not actually have much bearing on how important that term is, which can only be seen through human interpretation. So I guess my question is, how useful are these tools for study, actually?
